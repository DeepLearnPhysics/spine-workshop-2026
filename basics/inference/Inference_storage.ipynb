{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21dcae54-a1a7-45b3-847c-93630caa0270",
   "metadata": {},
   "source": [
    "# Full chain inference\n",
    "\n",
    "In this notebook, we will:\n",
    " * Take a look at what a complete ML reconstruction configuration look like\n",
    " * Understand what are the high-level represntations built out of the raw ML output\n",
    " * Understand how to visualize the data representations\n",
    " * Learn how to store the output of the ML reconstruction to file\n",
    " * Learn how to load an HDF5 file to back to a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21c115-f4aa-4202-8a75-36c405003392",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "***\n",
    "## 1. Full Chain Inference Configuration\n",
    "\n",
    "We start by pointing the python path to the reco chain package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fdc34-f7a3-430e-b269-f70e1f70561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOFTWARE_DIR = '/global/cfs/cdirs/m5252/software/spine' # Change this path to your software install\n",
    "\n",
    "# Set software directory\n",
    "sys.path.insert(0, SOFTWARE_DIR+'/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0290d-4d2c-4983-953f-258bfa12366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca1e1b-00f8-48a8-88ab-e0f477fb10a6",
   "metadata": {},
   "source": [
    "Now let's take a look at a full chain configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12aade-23ca-43d0-9b41-9a1869bd9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration file of the ML chain\n",
    "cfg_path = 'generic_full_chain.yaml'\n",
    "cfg = yaml.load(open(cfg_path, 'r'), Loader=yaml.Loader)\n",
    "\n",
    "print(yaml.dump(cfg, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccbc3e-9dd1-4464-8150-d79f77424c24",
   "metadata": {},
   "source": [
    "There's a lot to unpack. The details of each module in the full chain is beyond the scope of the WS, but the stucture is as presented in previous notebooks.\n",
    "\n",
    "You can focus on the `chain` configuration within the `model` block:\n",
    "\n",
    "```yaml\n",
    "      deghosting: null\n",
    "      charge_rescaling: null\n",
    "      segmentation: uresnet\n",
    "      point_proposal: ppn\n",
    "      fragmentation: graph_spice\n",
    "      shower_aggregation: grappa\n",
    "      shower_primary: grappa\n",
    "      track_aggregation: grappa\n",
    "      particle_aggregation: null\n",
    "      inter_aggregation: grappa\n",
    "      particle_identification: grappa\n",
    "      primary_identification: grappa\n",
    "      orientation_identification: grappa\n",
    "      calibration: null\n",
    "```\n",
    "\n",
    "You can see which network is performing which reconstruction task. You can note that a few modules are omitted here:\n",
    "- This a generic dataset, hence it has no ghost points (only relevant for wire TPCs)\n",
    "- The charge rescaling process is only applied when deghosting\n",
    "- The calibration is only relevant to correct for detector effects (none in this dataset)\n",
    "\n",
    "The rest of the model configuration is architectural details as to what parameters define the UResNets and the GNNs (GrapPAs). Here are the modules:\n",
    "- `uresnet_ppn`: Semantic segmentation + Point proposal\n",
    "- `graph_spice`: Fragmentation\n",
    "- `grappa_shower/track`: Aggregation of shower/track fragments\n",
    "- `grappa_inter`: Aggregation of particles, PID, primary and orientation predictions\n",
    "\n",
    "You can also see that the path to the weights for the full chain are provided under `model.weight_path`.\n",
    "\n",
    "```yaml\n",
    "  weight_path: /sdf/data/neutrino/generic/train/mpvmpr_2020_01_v04/weights/full_chain/default/snapshot-4999.ckpt\n",
    "```\n",
    "\n",
    "If you are executing this notebook anywhere but at S3DF, you must pull the weights from the path referenced on the notebook README.md and update the configuration accordingly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ab73d-563c-44ba-a67c-e67af3f8940d",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "***\n",
    "## 2. Running inference on one batch of data\n",
    "\n",
    "Once our ML model is fully trained and deployed, we may set our model to test mode and make use of its predictions: track vs shower separation, particle clustering, and PID to name a few. \n",
    " \n",
    "We first illustrate how to run the ML chain on one batch of data. Again, we use the main Driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa7c26-0e1c-46a0-b945-6cab4d28e0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spine.driver import Driver\n",
    "\n",
    "driver = Driver(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d8b34-49cd-42fa-b4d1-7b51a4560c6a",
   "metadata": {},
   "source": [
    "The following line runs one forward operation of the ML chain. It has one output:\n",
    "* `data`: Python dictionary containing inputs and outputs of the network\n",
    "  * 3d spacepoints, and deposition values)\n",
    "  * truth information used for labels\n",
    "  * Meta data information such as the image index number and px to cm conversion factor\n",
    "  * Various outputs of the reco chain (clusters, semantics, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cd12f-e040-4791-a1f5-b7c4e0bf29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = driver.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4636f-3f32-4fed-9e0a-0bc19d81262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of Data Dictionary =', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9071320-d719-440f-8b49-6d82dd3c11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf46b99-c5f4-44eb-86be-a10ca20c2a05",
   "metadata": {},
   "source": [
    "As you can see, the output of the full reconstruction chain is exhaustive, but arcane. This output is useful\n",
    "to debug each step of the reconstruction chain, but, as an analyzer, how do I interpret this information to\n",
    "build an analysis?\n",
    "\n",
    "Where are the particles? Where are the interactions?\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://media.tenor.com/onfpmM94llEAAAAe/the-dark-knight-christopher-nolan.png\" style=\"width:500px\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47a552-5397-4671-8afc-fca502a60437",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 3. Representation building\n",
    "\n",
    "Thankfully, this is where the (fragment), particle and interaction builders under\n",
    "[`spine.build`](https://github.com/DeepLearnPhysics/spine/tree/develop/spine/build)\n",
    "come in handy! Their purpose is to take the raw output of the full chain and convert\n",
    "it into human-reable representations. To construct these objects, all you need to do is\n",
    "to add the following block to the configuration:\n",
    "\n",
    "```yaml\n",
    "build:\n",
    "  mode: both\n",
    "  units: cm\n",
    "  fragments: false\n",
    "  particles: true\n",
    "  interactions: true\n",
    "```\n",
    "\n",
    "This is very simple:\n",
    "- `mode`: specifies whether the builders are to build reconstructed objects, truth reference objects, or both.\n",
    "  If you are running the reconstruction chain on data, you must set `mode` to `reco`.\n",
    "- `units`: units in which every coordinate must be expressed. Either `px` (native coordinate system of the input\n",
    "  or `cm`, detector coordinates, obtained from the meta information)\n",
    "- `fragments`: whether to build fragments or not (not useful for analysis, useful for debugging)\n",
    "- `particles`: whether to build particles or not\n",
    "- `ineractions`: whether to build interactions or not\n",
    "\n",
    "Now we can simply add this block to our initial configuration, run it and see what comes out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da0805-7c0e-410d-bb1c-6c4286114714",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load configuration file of the ML chain\n",
    "cfg_path = 'generic_full_chain.yaml'\n",
    "cfg = yaml.load(open(cfg_path, 'r'), Loader=yaml.Loader)\n",
    "cfg['build'] = {\n",
    "    'mode': 'both',\n",
    "    'units': 'cm',\n",
    "    'fragments': False,\n",
    "    'particles': True,\n",
    "    'interactions': True\n",
    "}\n",
    "\n",
    "driver = Driver(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3bd6e-9719-4546-a24e-079020b7dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = driver.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31816a98-c226-49fd-b086-82c4299b1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of Data Dictionary =', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5db0ad-4da9-4d0b-a7cf-8bab2cb66a7a",
   "metadata": {},
   "source": [
    "You can see that the number of data products has increased, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428468dd-7ba7-4b1a-9aff-dcd28fd34276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff7874-1cc5-4a80-b49c-088534e15091",
   "metadata": {},
   "source": [
    "At the end of the list, you can see that particle and interaction objects have now been added, phew!\n",
    "\n",
    "***\n",
    "\n",
    "Now let's visualize what these particle and interaction objects look like!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2e014-01b8-4d0b-9f6b-b4abd3f52ab6",
   "metadata": {},
   "source": [
    "Let's focus on the `*_particles` and `*_interactions` data products, which are locally-defined particle and interaction representations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e13337-6d86-4fd7-8d74-73508f5ae115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving data structures.\n",
    "# Here we need to index the data structures because we have process a batch!\n",
    "entry = 0\n",
    "reco_particles     = data['reco_particles'][entry]\n",
    "truth_particles    = data['truth_particles'][entry]\n",
    "reco_interactions  = data['reco_interactions'][entry]\n",
    "truth_interactions = data['truth_interactions'][entry]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb2c7a-1b30-47b7-87a9-ac40fc734ce8",
   "metadata": {},
   "source": [
    "Let us start by taking a look at the reco particle objects, what are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd225ab3-ee21-49e8-93d4-fa92516799e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_particles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15019085-551f-44b1-b1a0-dabca62a69a7",
   "metadata": {},
   "source": [
    "Here's a few particle attributes that might be useful:\n",
    "- `points`: Positions of the space points associated with the particle\n",
    "- `depositions`: Amount of charge/energy associated with each space point\n",
    "- `interaction_id`: ID of the interaction this particle belongs to\n",
    "- `pid`: Particle ID (see below for meaning of those numbers)\n",
    "- `is_primary`: Whether the particle originates from the primary vertex or not\n",
    "- `start_point`: Position of the start point\n",
    "- `end_point`: Position of the end point (for EM showers, same as start point)\n",
    "\n",
    "You may also notice that some of the reconstructed quantities are not filled (e.g. `start_dir`, `is_contained`, etc. This is because these attributes are filled by the post-processors, which have not yet ran...\n",
    "\n",
    "For a comprehensive list of available attributes, simply use `help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73ed34-0b77-49ac-84b4-d0f6965831eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(reco_particles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f942eb-67db-4145-bc92-e819aed492ab",
   "metadata": {},
   "source": [
    "Exercise: do the same for the `truth_particles` and investigate what is in there!\n",
    "\n",
    "We can do the same for interactions.\n",
    "\n",
    "This time we use the `as_dict()` method, which restricts the list of attributes to short-form attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59399741-5c34-4664-a996-23e55725819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_interactions[0].as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1491f0-5773-49d7-af66-03e0a0ed4c94",
   "metadata": {},
   "source": [
    "How do we fill the missing attributes? Let's take a look..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577b3da-2b2e-4177-9c53-772a9dfbc7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "***\n",
    "## 4. Post-processors\n",
    "\n",
    "Here a schematic representation of the data flow, after the execution of the full chain:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://github.com/francois-drielsma/lartpc_mlreco3d/raw/me/images/anatools.png\" style=\"width:800px\">\n",
    "</figure>\n",
    "\n",
    "The post-processing module under `spine.post` takes care of all non-ML reconstruction steps and is configured under the the `post` configuration block. Here is what the full post-processing suite would look on our generic dataset:\n",
    "\n",
    "```yaml\n",
    "post:\n",
    "  shape_logic:\n",
    "    enforce_pid: true\n",
    "    enforce_primary: true\n",
    "    priority: 3\n",
    "  direction:\n",
    "    obj_type: particle\n",
    "    optimize: true\n",
    "    run_mode: both\n",
    "    priority: 1\n",
    "  calo_ke:\n",
    "    scaling: 1.\n",
    "    shower_fudge: 1/0.83\n",
    "    priority: 1\n",
    "  csda_ke:\n",
    "    tracking_mode: step_next\n",
    "    segment_length: 5.0\n",
    "    priority: 1\n",
    "  mcs_ke:\n",
    "    tracking_mode: bin_pca\n",
    "    segment_length: 5.0\n",
    "    priority: 1\n",
    "  topology_threshold:\n",
    "    ke_thresholds:\n",
    "      4: 50\n",
    "      default: 25\n",
    "  vertex:\n",
    "    use_primaries: true\n",
    "    update_primaries: false\n",
    "    priority: 1\n",
    "  containment:\n",
    "    margin: 5.0\n",
    "    mode: meta\n",
    "  fiducial:\n",
    "    margin: 25.0\n",
    "    mode: meta\n",
    "  children_count:\n",
    "    mode: shape\n",
    "  match:\n",
    "    match_mode: both\n",
    "    ghost: false\n",
    "    fragment: false\n",
    "    particle: true\n",
    "    interaction: true\n",
    "```\n",
    "\n",
    "No need to understand in detail what each of these modules do. We will go over this again in detail in notebooks dedicated to performing these tasks or using the output of these post-processor. but it is intersting to check on our particle objects again now to see what is new..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808263a2-6977-4b1a-bb73-ef3aa3c5d709",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration file of the ML chain\n",
    "cfg_path = 'generic_full_chain_with_post.yaml'\n",
    "cfg = yaml.load(open(cfg_path, 'r'), Loader=yaml.Loader)\n",
    "\n",
    "print(yaml.dump(cfg, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fbf73-b7dc-4f2d-b1ac-9229a50b2100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver = Driver(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d1d09-7a1a-400b-92ee-cb5c3262d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = driver.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1193a-345f-4f99-8798-60c1dd9a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by getting the particles in the first entry of the batch\n",
    "\n",
    "entry = 0\n",
    "reco_particles = data['reco_particles'][entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d252a-9c19-44ff-ac64-72fe839ef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_particles[3].as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4cfba-d9c2-497b-985d-9abcf38a5e64",
   "metadata": {},
   "source": [
    "Now you can see that there's a lot more filled! E.g.:\n",
    "- `is_contained`: Whether the particle is contained within the volume of interest\n",
    "- `start_dir`/`end_dir`: Direction estimates w.r.t. to start/end points\n",
    "- `*_ke`: estimates using calorimetry, CSDA or MCS\n",
    "- `momentum`: estimate using `start_dir`, `ke` and `pid`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702fe13-55b1-4d33-b274-da16431904a0",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 5. Storage\n",
    "\n",
    "Ok, almost done eating your vegetables, I swear. One more step to have a fully fledged inference confuration! The `writer`.\n",
    "\n",
    "This block, which lives under `io`, defines what and how to store the output of the reco. chain + the post-processors to an HDF5 file. This is useful because it is significantly more efficient to run the full chain on a dataset to start and then use the output for analysis later. This centralizes the production process and saves time for the analyzers.\n",
    "\n",
    "The writer is defined under `spine.io.write.hdf5` and is configured as follows:\n",
    "\n",
    "```\n",
    "  writer:\n",
    "    name: hdf5\n",
    "    file_name: dummy.h5\n",
    "    overwrite: true\n",
    "    keys:\n",
    "      - run_info\n",
    "      - meta\n",
    "      - points\n",
    "      - points_label\n",
    "      - depositions\n",
    "      - depositions_label\n",
    "      - reco_particles\n",
    "      - truth_particles\n",
    "      - reco_interactions\n",
    "      - truth_interactions\n",
    "```\n",
    "\n",
    "This is the basics of what goes in there:\n",
    "- `name`: type of writer to use (currently HDF5 only)\n",
    "- `file_name`: name of the output file\n",
    "- `overwrite`: if `True` and the file already exists, it will be deleted and a new file will be created in its place\n",
    "- `keys`: list of keys in the `data` dictionary output that need to be stored to the HDF5 file. The list above is the exhaustive list of things that need to be stored to be able to restore the full SPINE `*particles` and `*interactions` objects from file. If you want to make a compact file with only top level information (no points, no depositions), simply comment out the following keys:\n",
    "  - `points*`\n",
    "  - `depositions*`\n",
    "  \n",
    "This block completes the assembly of a full chain inference configuration! These configurations are mainained on this repo:\n",
    "\n",
    "```html\n",
    "https://github.com/DeepLearnPhysics/spine_prod/\n",
    "```\n",
    "\n",
    "If you look under the `config` directory, you'll see that one full chain exists for each data modality this week:\n",
    "- `generic` (generic detector-less images, i.e. no detector sim.)\n",
    "- `icarus`\n",
    "- `sbnd`\n",
    "- `2x2`\n",
    "\n",
    "Let's load one of them and make a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dc740-57f6-42dc-8ccd-715b61b204a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spine.config import load_config_file\n",
    "\n",
    "# Load configuration file of the ML chain\n",
    "cfg_path = '/global/cfs/cdirs/m5252/software/spine-prod/config/infer/generic/full_chain_240805.yaml'\n",
    "cfg = load_config_file(cfg_path)\n",
    "\n",
    "cfg['io']['loader']['dataset']['file_keys'] = '/global/cfs/cdirs/m5252/dune/spine/workshop/larcv/generic_small.root'\n",
    "\n",
    "print(yaml.dump(cfg, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50ee65-02a8-40cc-9382-240c5065f3cd",
   "metadata": {},
   "source": [
    "Simply initialize the driver and call the `run` function (will run on the whole mini dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8626e00-e39f-4596-b455-d857ff7c9ed0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver = Driver(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84a191-3e12-4bef-affb-fad954e927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e356a7-76e1-4ecf-9d50-3e442d2f7448",
   "metadata": {},
   "source": [
    "You can now see that a new file has spawned (`dummy.h5`)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfbce7-a840-4b99-a892-5bd7a9ec6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6395fd8-1ea5-4719-bb71-862ac55f9470",
   "metadata": {},
   "source": [
    "In summary, provided with a full chain configuration and a set of weights, all you need to do to run inference is:\n",
    "\n",
    "```python\n",
    "import yaml\n",
    "from spine.driver import Driver\n",
    "\n",
    "driver = Driver(yaml.safe_load('/sdf/data/neutrino/software/spine_prod/config/generic/generic_full_chain_240718.cfg'))\n",
    "driver.run()\n",
    "```\n",
    "\n",
    "which is exactly equivalent to calling the following command:\n",
    "\n",
    "```bash\n",
    "python3 /path/to/spine/bin/run.py -c /sdf/data/neutrino/software/spine_prod/config/generic/generic_full_chain_240718.cfg\n",
    "```\n",
    "\n",
    "Typically this command would be run as part of batch job, as described in the training notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a7a4b-ff65-4123-8624-53b69fa8bbaf",
   "metadata": {},
   "source": [
    "In the next notebook, we'll discuss how to build an analysis with these files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5317f4-0301-4e8d-966b-f2d78ee46064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp_container",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
